# Work Notes: Alachua Civic Intelligence Reporting Studio

---

## Session: 2026-02-05 - Two-Layer Agent Architecture & Gemini Deep Research

**Session Focus:** Integrate Gemini Deep Research, implement two-layer agent architecture, scheduled pipeline runs

---

### Session Summary

Completed the two-layer agent architecture with dual research providers:
1. Created Gemini Deep Research client wrapper
2. Updated AnalystAgent to support Tavily + Gemini Deep Research
3. Integrated AnalystAgent into Orchestrator pipeline
4. Added 4 AM EST daily scheduled runs via Celery Beat
5. Added manual run UI in Orchestrator Panel
6. Comprehensive documentation updates

---

### Major Implementations

#### 1. Gemini Deep Research Client (`src/tools/gemini_research.py`)

New client wrapper for Google's Interactions API:
- Uses `deep-research-pro-preview-12-2025` agent
- Async polling with configurable timeout (default 5 min)
- `ResearchStatus` enum: PENDING, IN_PROGRESS, COMPLETED, FAILED
- `ResearchResult` dataclass with text, sources, status

```python
from src.tools.gemini_research import get_gemini_research_client

client = get_gemini_research_client()
result = client.research("Tara Forest development Alachua County", timeout_seconds=300)
```

#### 2. AnalystAgent Dual Research Providers

Updated `src/agents/analyst.py` with:
- `ResearchProvider` enum: TAVILY, GEMINI, BOTH
- `_research_with_tavily()` - Fast web search
- `_research_with_gemini()` - Comprehensive agentic research
- `_execute_research()` - Combines results from both providers

```python
from src.agents.analyst import AnalystAgent, ResearchProvider

# Use both providers (default)
analyst = AnalystAgent(name="Analyst", research_provider=ResearchProvider.BOTH)
```

#### 3. Orchestrator Pipeline Integration

Updated `src/orchestrator.py`:
- Added `AnalystAgent` initialization with configurable research provider
- Added `_run_deep_research()` method for Layer 2 analysis
- Triggers on high-relevance items (‚â•0.7 score)
- Added `skip_deep_research` parameter to `run_source()`

#### 4. Celery Beat Scheduling

Updated `src/tasks/celery_app.py`:
- Daily orchestrator pipeline at **4:00 AM EST**
- New `run_orchestrator_pipeline` task
- New `run_single_source` task for manual triggers
- Task routing to `orchestrator` queue

#### 5. Orchestrator Panel UI Enhancements

Updated `src/ui/pages/orchestrator_panel.py`:
- Added schedule info display (4 AM EST)
- Added "Skip Deep Research" checkbox
- Added "Run in Background (Celery)" option
- Shows task ID when queued for background execution

#### 6. Database Deep Research Methods

Updated `src/database.py`:
- `get_high_relevance_reports()` - Find reports needing deep research
- `save_deep_research_report()` - Save and link deep research results

---

### Files Created

| File | Lines | Purpose |
|:-----|:------|:--------|
| `src/tools/gemini_research.py` | 235 | Gemini Deep Research client |

### Files Modified

| File | Changes |
|:-----|:--------|
| `src/agents/analyst.py` | Added dual research providers |
| `src/orchestrator.py` | Added AnalystAgent integration, deep research |
| `src/tasks/celery_app.py` | Added 4 AM schedule, orchestrator tasks |
| `src/tasks/scout_tasks.py` | Added orchestrator pipeline tasks |
| `src/database.py` | Added deep research methods |
| `src/ui/pages/orchestrator_panel.py` | Added schedule info, async runs |
| `src/tools/__init__.py` | Export Gemini research classes |
| `requirements.txt` | Added google-genai==1.60.0 |
| `.env.example` | Documented research providers |

### Documentation Updated

| File | Changes |
|:-----|:--------|
| `docs/ARCHITECTURE.md` | Two-layer agent framework, updated diagrams |
| `docs/DEVELOPER_GUIDE.md` | Updated architecture section |
| `docs/SYSTEM_OVERVIEW.md` | Dual research providers |
| `docs/USER_GUIDE.md` | 4 AM schedule, Orchestrator Panel |
| `docs/PROJECT_PLAN.md` | Phase 3 complete, deliverables |
| `docs/PROJECT_MANAGEMENT.md` | E3 Analyst Layer complete |
| `TODO.md` | Added Feb 5 completed items |

---

### Architecture: Two-Layer Agent System

```mermaid
flowchart TB
    subgraph Orchestrator["üéØ ORCHESTRATOR (4 AM EST Daily)"]
        CC[CivicClerk<br/>Scraper]
        SR[SRWMD<br/>Scraper]
        FN[Florida Notices<br/>Scraper]
    end

    subgraph Layer1["üîç LAYER 1: ScoutAgent"]
        S1[Analyze scraped content]
        S2[Match against watchlist]
        S3[Generate relevance scores<br/>0.0 - 1.0]
    end

    subgraph Layer2["üß† LAYER 2: AnalystAgent"]
        TAV[Tavily Search<br/>fast, cheap]
        GEM[Gemini Deep Research<br/>thorough, agentic]
        COMB[Combined Deep<br/>Research Report]
        TAV --> COMB
        GEM --> COMB
    end

    CC --> Layer1
    SR --> Layer1
    FN --> Layer1
    Layer1 -->|"relevance ‚â• 0.7"| Layer2
```

---

### How to Use

**Start Scheduled Runs:**
```bash
# Terminal 1: Celery worker
celery -A src.tasks.celery_app worker --loglevel=info -Q orchestrator,scouts

# Terminal 2: Celery Beat scheduler
celery -A src.tasks.celery_app beat --loglevel=info
```

**Manual Runs:**
```python
from src.tasks.scout_tasks import run_orchestrator_pipeline

# Queue full pipeline
run_orchestrator_pipeline.delay(force=True)
```

---

### Next Steps

1. Test full pipeline with real data
2. Add integration tests for deep research
3. Implement human-in-the-loop approval flow
4. Add email notifications for high-relevance items

---
---

## Session: 2026-02-02 - Phase 3 Intelligence Layer

**Session Focus:** Intelligent Evolution - Event-driven architecture, CivicEvent model, adapters, EventStore, Watchdog Rules Engine

---

### Session Summary

Implemented Phase 3 "Intelligent Evolution" with an event-driven, user-centric architecture:
1. Created unified CivicEvent model normalizing all scraper output
2. Built source adapters for CivicClerk, SRWMD, and Florida Notices
3. Implemented EventStore with persistence and query capabilities
4. Created Watchdog Rules Engine with 14 configurable civic alert rules
5. Integrated ResourceCache into all 3 scrapers
6. Created source playbook generator and sitemap discovery scripts
7. **78 tests passing** (39 scraper + 39 intelligence)

---

### Major Implementations

#### 1. Intelligence Layer (`src/intelligence/`)

New module providing event-driven intelligence capabilities:

```
src/intelligence/
‚îú‚îÄ‚îÄ __init__.py              # Module exports
‚îú‚îÄ‚îÄ models.py                # CivicEvent, Entity, Document, Alert (~300 lines)
‚îú‚îÄ‚îÄ event_store.py           # Persistence + queries (~400 lines)
‚îú‚îÄ‚îÄ rules_engine.py          # Watchdog alerts (~400 lines)
‚îî‚îÄ‚îÄ adapters/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ base_adapter.py      # Base class with entity/tag extraction
    ‚îú‚îÄ‚îÄ civicclerk_adapter.py
    ‚îú‚îÄ‚îÄ srwmd_adapter.py
    ‚îî‚îÄ‚îÄ florida_notices_adapter.py
```

**CivicEvent Model:**
```python
@dataclass
class CivicEvent:
    event_id: str
    event_type: EventType      # meeting, permit_application, permit_issued, public_notice
    source_id: str
    timestamp: datetime
    title: str
    description: Optional[str]
    location: Optional[GeoLocation]
    entities: List[Entity]     # People, orgs, addresses
    documents: List[Document]
    tags: List[str]            # For filtering
    content_hash: str          # For change detection
    raw_data: Dict
```

#### 2. EventStore (`src/intelligence/event_store.py`)

Persistent storage with query capabilities:
- `save_event()` - Detects new vs updated vs unchanged
- `get_events()` - Filter by source, type, tags, time range
- `get_whats_new()` - Events discovered in last N hours
- `get_upcoming()` - Future events (meetings, hearings)
- `get_by_entity()` - Find events mentioning an entity
- `get_by_county()` - Filter by county

#### 3. Watchdog Rules Engine (`src/intelligence/rules_engine.py`)

Rule-based alert generation for civic watchdog use cases:
- Loads rules from `config/watchdog_rules.yaml`
- Evaluates CivicEvents against configurable conditions
- Generates alerts with severity levels (info, notable, warning, urgent)

**14 Default Rules:**
| Category | Rules |
|:---------|:------|
| Permits | new-alachua-permit, permit-issued-alachua |
| Rezoning | rezoning-alert, comprehensive-plan-change, variance-request |
| Environmental | environmental-concern, santa-fe-river |
| Meetings | upcoming-meeting-48h, public-hearing, planning-commission |
| Development | large-development, annexation |
| Notices | legal-notice-alachua |

#### 4. Source Adapters

Convert scraper output to unified CivicEvent format:
- **CivicClerkAdapter**: Meetings ‚Üí CivicEvents with board entities, agenda documents
- **SRWMDAdapter**: Permits ‚Üí CivicEvents with project entities, location, permit type tags
- **FloridaNoticesAdapter**: Notices ‚Üí CivicEvents with county location, PDF documents

#### 5. ResourceCache Integration

All 3 scrapers now auto-update discovered resources cache:
- `config/discovered_resources.yaml` - Persistent cache of known IDs
- `src/tools/resource_cache.py` - Read/write utility
- CivicClerk: 20 event IDs cached
- SRWMD: Permit IDs and document IDs cached
- Florida Notices: Notice IDs and PDF URLs cached

#### 6. Source Discovery Scripts

- `scripts/discover_sitemaps.py` - Firecrawl map_site API integration
- `scripts/analyze_sources.py` - Deep content sampling, playbook generation
- `config/source_playbooks/` - 5 YAML playbooks generated

---

### Architecture Decision

**Chose Option C: Event-driven + User-centric hybrid**

Instead of complex layered infrastructure, we built:
1. **Unified CivicEvent model** - Single query interface across all sources
2. **Change detection at scrape time** - Not batch snapshot comparison
3. **Rule-based alerts** - Explicit, auditable, citizen-understandable
4. **Simpler self-healing** - Health as scraper property, not separate system

**Primary Use Case:** Grassroots civic watchdog monitoring for concerning activity

---

### Files Created

| File | Lines | Purpose |
|:-----|:------|:--------|
| `src/intelligence/__init__.py` | 40 | Module exports |
| `src/intelligence/models.py` | 300 | CivicEvent, Entity, Document, Alert |
| `src/intelligence/event_store.py` | 400 | Event persistence and queries |
| `src/intelligence/rules_engine.py` | 400 | Watchdog alert generation |
| `src/intelligence/adapters/__init__.py` | 15 | Adapter exports |
| `src/intelligence/adapters/base_adapter.py` | 140 | Base adapter with entity extraction |
| `src/intelligence/adapters/civicclerk_adapter.py` | 250 | CivicClerk ‚Üí CivicEvent |
| `src/intelligence/adapters/srwmd_adapter.py` | 280 | SRWMD ‚Üí CivicEvent |
| `src/intelligence/adapters/florida_notices_adapter.py` | 270 | Florida Notices ‚Üí CivicEvent |
| `config/watchdog_rules.yaml` | 150 | 14 civic watchdog rules |
| `config/discovered_resources.yaml` | 60 | Resource cache |
| `src/tools/resource_cache.py` | 170 | Cache utility |
| `scripts/discover_sitemaps.py` | 645 | Sitemap discovery |
| `scripts/analyze_sources.py` | 872 | Playbook generator |
| `test/test_intelligence.py` | 400 | 39 intelligence tests |

### Files Modified

| File | Changes |
|:-----|:--------|
| `src/tools/civicclerk_scraper.py` | Added ResourceCache integration |
| `src/tools/srwmd_scraper.py` | Added ResourceCache integration |
| `src/tools/florida_notices_scraper.py` | Added ResourceCache integration |
| `src/tools/__init__.py` | Export ResourceCache |
| `TODO.md` | Added Phase 3 items |
| `docs/PROJECT_PLAN.md` | Added Phase 3 architecture |

---

### Test Results

```
78 passed in 2.54s
- 39 scraper tests
- 39 intelligence tests (21 model + 10 store + 8 rules)
```

---

### Phase 3 Progress

| Phase | Status | Description |
|:------|:-------|:------------|
| **3.1** | ‚úÖ Complete | CivicEvent model + adapters |
| **3.2** | ‚úÖ Complete | EventStore + queries |
| **3.3** | ‚úÖ Complete | Watchdog rules engine |
| **3.4** | üî≤ Pending | Health metrics in scrapers |
| **3.5** | üî≤ Pending | User watchlists |
| **3.6** | üî≤ Pending | Entity extraction for linking |
| **3.7** | üî≤ Pending | Cross-source search |

---

### Next Steps

1. Phase 3.4: Health metrics embedded in scrapers
2. Phase 3.5: User watchlists
3. Comprehensive documentation update

---
---

## Session: 2026-02-01 - Phase 2 Complete + Code Review Fixes

**Session Focus:** Hybrid Scraping Pipeline, SRWMD Scraper, Orchestrator, Code Review

---

### Session Summary

Completed Phase 2 of the project including:
1. Hybrid scraping pipeline with Discovery + Detail phases
2. SRWMD permit scraper (applications + issuances + E-Permitting detail)
3. Central Orchestrator for pipeline coordination
4. Comprehensive code review and fixes
5. Streamlit UI improvements

---

### Major Implementations

#### 1. SRWMD Permit Scraper (`src/tools/srwmd_scraper.py`)
- **767 lines** of new code
- Scrapes permit applications from `/1616/Notice-of-Receipt-of-Applications`
- Scrapes permit issuances from `/1617/Notice-of-Permit-Issuance`
- E-Permitting portal detail page scraping with document list extraction
- County filtering (Alachua focus)
- Data classes: `PermitNotice`, `PermitDetail`, `PermitDocument`

#### 2. Orchestrator (`src/orchestrator.py`)
- **630 lines** - Central pipeline coordinator
- Job scheduling based on source frequency
- Source-specific job runners (CivicClerk, Florida Notices, SRWMD)
- Scout Agent integration for analysis
- Pipeline run tracking with `JobResult` and `PipelineRun` dataclasses
- CLI interface for command-line execution

#### 3. Orchestrator UI (`src/ui/pages/orchestrator_panel.py`)
- Dashboard with source status
- Run Pipeline tab with source selection
- History tab for past runs
- Fixed nested expander Streamlit bug

#### 4. Source Configuration (`config/sources.yaml`)
- Added `srwmd-permit-applications` (critical, daily)
- Added `srwmd-permit-issuances` (critical, daily)
- Updated with custom scraper references

#### 5. Streamlit Sidebar Cleanup
- Removed redundant auto-generated navigation
- Added Target Data Sources section with link buttons
- Created `.streamlit/config.toml` with `showSidebarNavigation = false`

---

### Code Review Fixes

#### High Priority (Fixed)

1. **Removed unused `asyncio` import** in `src/orchestrator.py`
2. **Fixed hardcoded `site_id`** - Now extracted from source URL config
3. **Added URL validation** in `src/tools/firecrawl_client.py`:
   - `ALLOWED_DOMAINS` allowlist for security
   - `_validate_url()` method
   - Validation in `scrape_page()`, `scrape_pdf()`, `map_site()`
4. **Added `SourceType` constants** for consistent source matching

#### Medium Priority (Noted for future)

- Add scraper unit tests with mocked responses
- Add database FK constraints
- Implement async/parallel scraping
- Add connection pooling for production

---

### Files Created

| File | Lines | Purpose |
|:-----|:------|:--------|
| `src/tools/srwmd_scraper.py` | 767 | SRWMD permit scraper |
| `src/orchestrator.py` | 630 | Pipeline orchestrator |
| `src/ui/pages/orchestrator_panel.py` | 292 | Orchestrator UI |
| `.streamlit/config.toml` | 9 | Streamlit config |

### Files Modified

| File | Changes |
|:-----|:--------|
| `config/sources.yaml` | Added SRWMD sources |
| `src/ui/app.py` | Added Orchestrator tab, updated sidebar |
| `src/ui/pages/source_tester.py` | Added SRWMD test function |
| `src/tools/firecrawl_client.py` | Added URL validation |
| `migrations/001_scraped_meetings.sql` | Already existed, reviewed |

---

### Target Data Sources (Active)

| Source | URL | Scraper | Status |
|:-------|:----|:--------|:-------|
| CivicClerk | `https://alachuafl.portal.civicclerk.com/` | `CivicClerkScraper` | ‚úÖ Ready |
| Florida Public Notices | `https://floridapublicnotices.com/` | `FloridaNoticesScraper` | ‚úÖ Ready |
| SRWMD Applications | `https://www.mysuwanneeriver.com/1616/` | `SRWMDScraper` | ‚úÖ Ready |
| SRWMD Issuances | `https://www.mysuwanneeriver.com/1617/` | `SRWMDScraper` | ‚úÖ Ready |

---

### Next Steps

1. Add scraper unit tests
2. Test full pipeline with real data
3. Phase 3: Intelligent Evolution (learning, pattern detection)

---
---

## Session: 2026-01-29 (Previous)

**Session Focus:** P0 and P1 Bug Fixes and Core Implementation

---

## Session Summary

This session addressed all critical (P0) and high-priority (P1) issues identified in the code review. The codebase is now aligned with the documented architecture in the README.

---

## Changes Made

### üî¥ P0 - Critical Fixes

#### 1. `src/tools.py` - Replaced Broken Dependencies
**Before:** Used `requests` and `BeautifulSoup` (removed from requirements)
**After:** Implemented Firecrawl-based scraping with:
- Lazy client initialization
- `monitor_url()` - Scrapes web pages with JS rendering
- `scrape_pdf()` - Extracts text from PDF URLs
- `deep_research()` - Tavily search integration

#### 2. `src/models.py` - Fixed Invalid Model Names
**Before:** `gemini-3.0-pro`, `gemini-3.0-flash` (don't exist)
**After:** `gemini-2.5-pro`, `gemini-2.5-flash`
- Added lazy API key loading
- Added docstrings

#### 3. `src/config.py` - Removed Module-Level Crash
**Before:** Raised `ValueError` on import if `GOOGLE_API_KEY` missing
**After:** Removed validation at import time
- Added `CELERY_BROKER_URL` and `CELERY_RESULT_BACKEND` exports
- Legacy exports now have warning comments

---

### üü† P1 - Core Implementation

#### 4. Deleted `src/registry.py`
**Reason:** Duplicated `config/sources.yaml`
**Updated:** `src/main.py` to use `config.get_sources_by_priority()`

#### 5. Created `src/tools/` Package
New files:
- `src/tools/__init__.py`
- `src/tools/firecrawl_client.py` - Full Firecrawl wrapper with:
  - Retry logic with exponential backoff
  - `scrape_page()` - JS-rendered pages
  - `scrape_pdf()` - PDF extraction
  - `scrape_civicclerk()` - CivicClerk-specific scraper
  - `map_site()` - URL discovery
  - `batch_scrape()` - Multiple URLs
- `src/tools/docling_processor.py` - Document processing with:
  - `process_file()` - Local files
  - `process_url()` - Remote documents
  - `process_bytes()` - In-memory content
  - `extract_tables()` - Table extraction
  - `chunk_text()` - LangChain text splitting

#### 6. Created `src/app.py` - FastAPI Application
Endpoints:
- `GET /` - Health check
- `GET /health` - Health status
- `GET /info` - Instance information
- `POST /run` - Start agent run
- `GET /status/{run_id}` - Check run status
- `GET /runs` - List recent runs
- `GET /approvals/pending` - List pending approvals
- `GET /approvals/{id}` - Get approval details
- `POST /approvals/{id}/decide` - Approve/reject
- `GET /stream/{run_id}` - SSE streaming

#### 7. Created `src/tasks/` Package - Celery Integration
New files:
- `src/tasks/__init__.py`
- `src/tasks/celery_app.py` - Celery configuration with:
  - Redis broker/backend
  - Beat schedule (daily scouts, weekly analysts, monthly synthesizers)
  - Task routing by queue
- `src/tasks/scout_tasks.py` - Scout task definitions:
  - `run_scout()` - Single URL
  - `run_all_critical_scouts()` - All critical sources
  - `run_scout_for_source()` - By source ID

#### 8. Created `src/api/routes/` Package
New files:
- `src/api/__init__.py`
- `src/api/routes/__init__.py`
- `src/api/routes/workflows.py` - Workflow endpoints
- `src/api/routes/approvals.py` - Approval endpoints

#### 9. Created `src/workflows/` Package - LangGraph
New files:
- `src/workflows/__init__.py`
- `src/workflows/graphs.py` - Workflow definitions:
  - `ScoutState` / `AnalystState` - TypedDict states
  - `create_scout_workflow()` - fetch ‚Üí analyze ‚Üí save
  - `create_analyst_workflow()` - gather ‚Üí synthesize ‚Üí [approve] ‚Üí publish
  - `run_scout_workflow()` / `run_analyst_workflow()` - Runners

---

## Files Created

```
src/
‚îú‚îÄ‚îÄ app.py                    # NEW - FastAPI application
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py          # NEW
‚îÇ   ‚îú‚îÄ‚îÄ firecrawl_client.py  # NEW - Firecrawl wrapper
‚îÇ   ‚îî‚îÄ‚îÄ docling_processor.py # NEW - Document processing
‚îú‚îÄ‚îÄ tasks/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py          # NEW
‚îÇ   ‚îú‚îÄ‚îÄ celery_app.py        # NEW - Celery configuration
‚îÇ   ‚îî‚îÄ‚îÄ scout_tasks.py       # NEW - Scout tasks
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py          # NEW
‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py      # NEW
‚îÇ       ‚îú‚îÄ‚îÄ workflows.py     # NEW - Workflow routes
‚îÇ       ‚îî‚îÄ‚îÄ approvals.py     # NEW - Approval routes
‚îî‚îÄ‚îÄ workflows/
    ‚îú‚îÄ‚îÄ __init__.py          # NEW
    ‚îî‚îÄ‚îÄ graphs.py            # NEW - LangGraph workflows
```

## Files Modified

- `src/tools.py` - Replaced requests/bs4 with Firecrawl
- `src/models.py` - Fixed model names, lazy loading
- `src/config.py` - Removed import-time crash
- `src/main.py` - Updated to use YAML config

## Files Deleted

- `src/registry.py` - Redundant, replaced by YAML config

---

## Current Project Structure

```
src/
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ base.py
‚îÇ   ‚îú‚îÄ‚îÄ scout.py
‚îÇ   ‚îî‚îÄ‚îÄ analyst.py
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ workflows.py
‚îÇ       ‚îî‚îÄ‚îÄ approvals.py
‚îú‚îÄ‚îÄ tasks/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ celery_app.py
‚îÇ   ‚îî‚îÄ‚îÄ scout_tasks.py
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ firecrawl_client.py
‚îÇ   ‚îî‚îÄ‚îÄ docling_processor.py
‚îú‚îÄ‚îÄ workflows/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ graphs.py
‚îú‚îÄ‚îÄ app.py           # FastAPI application
‚îú‚îÄ‚îÄ config.py        # Configuration loader
‚îú‚îÄ‚îÄ database.py      # Supabase client
‚îú‚îÄ‚îÄ main.py          # CLI entry point
‚îú‚îÄ‚îÄ models.py        # LLM configuration
‚îú‚îÄ‚îÄ schemas.py       # Pydantic models
‚îî‚îÄ‚îÄ tools.py         # LangChain tools
```

---

## Next Steps (P2 Items)

1. Fix `src/database.py` - Lazy initialization
2. Create additional schemas (AnalystReport, SynthesizerReport)
3. Fix `src/agents/base.py` - Return type
4. Create `docs/DEVELOPER_GUIDE.md`
5. Add embedding/vector support

---

## Running the Application

### FastAPI Server
```bash
uvicorn src.app:app --reload --port 8000
```

### Celery Worker
```bash
celery -A src.tasks.celery_app worker --loglevel=info
```

### Celery Beat (Scheduler)
```bash
celery -A src.tasks.celery_app beat --loglevel=info
```

### CLI (Legacy)
```bash
python -m src.main --agent A1 --url https://example.com
```

---

## Notes

- All P0 and P1 items completed
- Codebase now matches README architecture
- In-memory state used for runs/approvals (replace with Redis/DB in production)
- LangGraph workflows use MemorySaver (replace with Supabase checkpointer in production)

---

## Session 2: Project Documentation (2026-01-29 Evening)

### Documents Created

#### 1. `docs/PROJECT_PLAN.md`
High-level project plan and roadmap including:
- Executive summary and vision
- Timeline with 4 phases (Foundation ‚Üí Scout ‚Üí Analyst ‚Üí Integration)
- Phase breakdown with deliverables and milestones
- Release plan (v1.0, v1.1, v2.0)
- Resource requirements and cost estimates
- Risk assessment
- Success metrics

#### 2. `docs/SPEC.md`
Technical specification document including:
- System architecture diagrams
- Technology stack details
- Data models and schemas (Pydantic + SQL)
- API specification (all endpoints)
- Agent specifications (Scout/Analyst workflows)
- Configuration requirements
- Security considerations
- Performance requirements
- Deployment instructions

#### 3. `docs/PROJECT_MANAGEMENT.md`
Azure DevOps-style project tracking including:
- 5 Epics (E1-E5)
- 15+ Features with user stories
- 50+ Tasks with estimates
- Sprint backlog
- Velocity tracking
- Risk register
- Decision log
- Meeting notes section

### Project Name
**Confirmed:** Open Sousveillance Studio (updated in PROJECT_PLAN.md)

### Key Dates
| Milestone | Target Date |
|:----------|:------------|
| Phase 2 Complete | Feb 15, 2026 |
| Phase 3 Complete | Mar 15, 2026 |
| v1.0 Release | Apr 1, 2026 |

### Next Actions
1. Begin CivicClerk scraper implementation (T2.2.1-T2.2.6)
2. Implement change detection (T2.6.1-T2.6.4)
3. Write integration tests for Scout layer

---

## Session 3: Testing, Security & Dev Console (2026-01-30)

### Session Focus
- Codebase review and testing
- Security vulnerability scanning
- Streamlit Dev Console implementation
- Native Google GenAI SDK migration

### Major Accomplishments

#### 1. Security Scan & Fixes
- Ran `pip-audit`, `bandit`, and `safety` scans
- Fixed hardcoded `0.0.0.0` host binding ‚Üí environment variable
- Pinned all dependencies to specific versions in `requirements.txt`
- Added `HOST`, `PORT`, `RELOAD` env vars

#### 2. Streamlit Dev Console
Created browser-based testing interface at `src/ui/`:

```
src/ui/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ app.py                    # Main Streamlit app
‚îî‚îÄ‚îÄ pages/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ agent_runner.py       # Test Scout/Analyst agents
    ‚îú‚îÄ‚îÄ prompt_inspector.py   # View prompt library
    ‚îú‚îÄ‚îÄ source_tester.py      # Test web scraping
    ‚îî‚îÄ‚îÄ config_viewer.py      # View YAML configs
```

**Run with:** `streamlit run src/ui/app.py`

#### 3. Native Google GenAI SDK Migration
Replaced `langchain_google_genai` with native `google.genai` SDK to fix PyTorch DLL loading issues on Windows:

- `src/models.py` - New `GeminiModel` and `StructuredGeminiModel` classes
- `src/agents/scout.py` - Updated to use native SDK
- `src/agents/analyst.py` - Updated to use native SDK
- Fixed model names: `gemini-2.5-pro`, `gemini-2.5-flash`

#### 4. Test Suite Fixes
- Fixed NumPy version conflict (`numpy<2` for docling)
- Fixed test mocks to match actual implementations
- Cleaned Celery beat schedule (removed non-existent tasks)
- **Result:** 37 tests passing, 7 skipped

### Files Created
- `src/ui/__init__.py`
- `src/ui/app.py`
- `src/ui/pages/__init__.py`
- `src/ui/pages/agent_runner.py`
- `src/ui/pages/prompt_inspector.py`
- `src/ui/pages/source_tester.py`
- `src/ui/pages/config_viewer.py`
- `pytest.ini`

### Files Modified
- `requirements.txt` - Pinned versions, added streamlit
- `src/models.py` - Native google.genai SDK
- `src/agents/scout.py` - Removed LangChain dependency
- `src/agents/analyst.py` - Removed LangChain dependency
- `src/tools/__init__.py` - Export tools, catch OSError
- `src/app.py` - Environment variable for host binding
- `.env.example` - Added HOST, PORT, RELOAD vars

### Current Architecture

```mermaid
flowchart TB
    subgraph UI["üî¨ Dev Console"]
        ST[Streamlit<br/>localhost:8501]
    end

    subgraph Agents["ü§ñ Agents"]
        SA[ScoutAgent]
        AA[AnalystAgent]
    end

    subgraph LLM["üß† LLM"]
        GEM[google.genai<br/>Gemini 2.5 Pro]
    end

    subgraph Tools["üîß Tools"]
        FC[Firecrawl]
        TAV[Tavily]
    end

    ST --> SA & AA
    SA --> GEM
    SA --> FC
    AA --> GEM
    AA --> TAV
```

### Test Results
```
37 passed, 7 skipped in 11.43s
```

### Next Session Priorities
1. Supabase integration testing
2. Human-in-the-loop approval flow
3. Florida Public Notices scraper

---

## Session 3 Continued: Documentation & Standards (2026-01-30 Afternoon)

### Focus
- Documentation restructuring
- Coding standards creation
- Project planning expansion

### Major Accomplishments

#### 1. Documentation Restructure
- Simplified `README.md` to ~180 lines (was ~750)
- Created `docs/ARCHITECTURE.md` with all technical diagrams
- Created `docs/SYSTEM_OVERVIEW.md` for workflow brainstorming

#### 2. Project Plan Expansion
Updated `docs/PROJECT_PLAN.md` with:
- **12 new agents** across all layers (A5-A8, B3-B5, C5-C7, R1-R3)
- **40+ features** across 9 categories
- **6 Epics** with user stories and task breakdowns
- **~200 hours** of estimated work

#### 3. Coding Standards
Created `docs/CODING_STANDARDS.md` (~700 lines) covering:
- Python style guide (PEP 8 + project specifics)
- Agent development standards
- Testing requirements
- Git workflow (Conventional Commits)
- Security standards
- AI assistant guidelines

#### 4. Tooling Configuration
- `pyproject.toml` - Black, Ruff, mypy, pytest settings
- `.pre-commit-config.yaml` - Automated quality checks
- `.vscode/settings.json` - IDE workspace settings
- `src/exceptions.py` - Custom exception hierarchy

### Files Created
- `docs/ARCHITECTURE.md`
- `docs/SYSTEM_OVERVIEW.md`
- `docs/CODING_STANDARDS.md`
- `pyproject.toml`
- `.pre-commit-config.yaml`
- `.vscode/settings.json`
- `src/exceptions.py`

### Files Modified
- `README.md` - Simplified, added new doc links
- `TODO.md` - Added completed items, updated file status
- `docs/PROJECT_PLAN.md` - Added agents, features, epics
- `docs/DEVELOPER_GUIDE.md` - Added Streamlit section

### Documentation Structure

```
docs/
‚îú‚îÄ‚îÄ ARCHITECTURE.md        # Technical diagrams, system design
‚îú‚îÄ‚îÄ CODING_STANDARDS.md    # Development standards (NEW)
‚îú‚îÄ‚îÄ DEVELOPER_GUIDE.md     # Setup, contributing
‚îú‚îÄ‚îÄ PROJECT_PLAN.md        # Roadmap, epics, user stories
‚îú‚îÄ‚îÄ SYSTEM_OVERVIEW.md     # Workflow brainstorming (NEW)
‚îú‚îÄ‚îÄ SPEC.md                # Technical specification
‚îî‚îÄ‚îÄ ...other docs
```

### Session Summary
All core documentation is now in place for open-source contributors:
- Clear README for quick start
- Comprehensive coding standards for consistency
- Detailed project plan with prioritized backlog
- Architecture docs with mermaid diagrams
